# Sistema de Chat con PDFs - DocumentaciÃ³n TÃ©cnica (MEJORADO)

## ğŸ—ï¸ Arquitectura del Sistema Mejorado

### Flujo Completo de Procesamiento (VersiÃ³n 2.0)

```
PDF Escaneado â†’ OCR â†’ Texto â†’ InyecciÃ³n Constante â†’ Gemini â†’ Respuesta
     â†“              â†“         â†“                      â†“         â†“
  233KB PDF    1,660 chars  PDF en CADA mensaje    IA        Usuario
                            + Monitoreo Tokens
```

### âœ¨ Mejoras Implementadas

- **âœ… InyecciÃ³n Constante del PDF**: El contenido del PDF se incluye en TODOS los mensajes
- **âœ… Monitoreo de Tokens**: Seguimiento en tiempo real del uso de tokens
- **âœ… GestiÃ³n de Sesiones**: Sistema robusto de manejo de sesiones
- **âœ… Persistencia de Contexto**: GarantÃ­a de que el PDF nunca se "olvida"

## ğŸ“„ 1. Procesamiento de PDFs

### ExtracciÃ³n de Texto
```python
# pdf_processor.py
def extract_text(pdf_path) -> Tuple[str, str]:
    # 1. Intenta extracciÃ³n estÃ¡ndar (PDFs con texto)
    # 2. Si falla, usa OCR.space API (PDFs escaneados)
    # 3. Retorna (texto_extraÃ­do, mÃ©todo_usado)
```

**Resultado actual:**
- **Entrada**: PDF escaneado de 233KB, 1 pÃ¡gina
- **Salida**: 1,660 caracteres de texto limpio
- **MÃ©todo**: OCR (porque es escaneado)

## ğŸ¤– 2. Sistema de Prompts y Contexto (MEJORADO)

### Estructura del Prompt (Sin Cambios)

```python
# llm_client.py - get_system_prompt()
def get_system_prompt(pdf_content=None):
    base_prompt = """
    RecibirÃ¡s documentos PDF con informaciÃ³n formal.
    Tu tarea es resumir y explicar el contenido en lenguaje claro...
    """

    if pdf_content:
        return f"""
        {base_prompt}

        CONTENIDO DEL DOCUMENTO PDF:
        ---
        {pdf_content}  # â† AQUÃ SE INYECTA TODO EL PDF
        ---

        Ahora puedes responder preguntas sobre este documento...
        """
```

### âœ¨ GestiÃ³n de Conversaciones MEJORADA

```python
# llm_client.py - chat() MEJORADO
async def chat(message, pdf_content=None, conversation_history=None):
    # 1. Monitoreo de tokens NUEVO
    token_info = self.get_token_usage_info(message, pdf_content, conversation_history)
    logger.info(f"Token usage: {token_info['total_tokens']:,} tokens")

    # 2. Agregar historial de conversaciÃ³n
    chat_history = []
    if conversation_history:
        for msg in conversation_history:
            chat_history.append({'role': msg['role'], 'parts': [msg['content']]})

    # 3. INYECCIÃ“N CONSTANTE DEL PDF (CAMBIO PRINCIPAL)
    if pdf_content:
        system_prompt_with_pdf = self.get_system_prompt(pdf_content)
        full_message = f"{system_prompt_with_pdf}\n\nUsuario: {message}"
        logger.info("PDF content injected into message for persistent context")
    else:
        full_message = message

    # 4. Enviar a Gemini
    chat = self.model.start_chat(history=chat_history)
    response = chat.send_message(full_message)
```

### ğŸ”„ Nueva Clase PDFChatSession

```python
# pdf_chat_session.py - NUEVO
class PDFChatSession:
    def __init__(self):
        self.pdf_content = None          # PDF cargado
        self.conversation_history = []   # Historial completo
        self.total_tokens_used = 0      # Monitoreo de tokens
        self.token_usage_history = []   # Historial de uso

    async def chat(self, message):
        # SIEMPRE inyecta PDF + monitorea tokens + actualiza historial
        response = await self.llm_client.chat(
            message=message,
            pdf_content=self.pdf_content,  # â† SIEMPRE presente
            conversation_history=self.conversation_history
        )
        return {'response': response, 'token_info': {...}}
```

## ğŸ”„ 3. Flujo de SesiÃ³n MEJORADO

### âœ¨ SesiÃ³n = 1 PDF + ConversaciÃ³n + Monitoreo

```
SesiÃ³n Iniciada (PDFChatSession)
    â†“
PDF cargado â†’ Texto extraÃ­do â†’ Almacenado en sesiÃ³n
    â†“
Usuario pregunta 1 â†’ PDF + Pregunta â†’ Respuesta 1 + Token tracking
    â†“
Usuario pregunta 2 â†’ PDF + Pregunta + Historial â†’ Respuesta 2 + Token tracking
    â†“
Usuario pregunta N â†’ PDF + Pregunta + Historial â†’ Respuesta N + Token tracking
                     â†‘
            INYECCIÃ“N CONSTANTE
```

### âœ… GestiÃ³n de Memoria SOLUCIONADA

**Problema RESUELTO**: El PDF se inyecta en **TODOS los mensajes**.

**Ventajas**:
- âœ… Contexto PDF garantizado en cada respuesta
- âœ… No hay pÃ©rdida de informaciÃ³n del documento
- âœ… Monitoreo completo de tokens
- âœ… GestiÃ³n robusta de sesiones

**Costo**: Ligeramente mÃ¡s tokens por mensaje (pero dentro de lÃ­mites seguros)

## ğŸ“Š 4. AnÃ¡lisis de Tokens y LÃ­mites

### CÃ¡lculo Aproximado de Tokens

```
Prompt base:           ~200 tokens
PDF content (1,660 chars): ~415 tokens (1 token â‰ˆ 4 chars)
ConversaciÃ³n (8 mensajes): ~800 tokens
TOTAL APROXIMADO:      ~1,415 tokens
```

**LÃ­mite de Gemini 2.0 Flash**: 1M tokens de entrada
**Nuestro uso actual**: 0.14% del lÃ­mite âœ…

### ProyecciÃ³n para MÃºltiples PDFs

```
1 PDF (1,660 chars):     ~415 tokens
5 PDFs similares:        ~2,075 tokens
10 PDFs similares:       ~4,150 tokens
ConversaciÃ³n larga:      ~2,000 tokens
TOTAL (10 PDFs):         ~6,150 tokens (0.6% del lÃ­mite)
```

## ğŸ¯ 5. Limitaciones Actuales

### âŒ Problemas Identificados

1. **InyecciÃ³n Ãºnica**: PDF solo se inyecta en primer mensaje
2. **Sin segmentaciÃ³n**: No hay separaciÃ³n clara entre PDFs
3. **Sin gestiÃ³n de tokens**: No monitoreamos el uso
4. **SesiÃ³n simple**: Solo 1 PDF por sesiÃ³n

### âš ï¸ Riesgos Potenciales

1. **Conversaciones largas**: Gemini podrÃ­a perder contexto del PDF
2. **PDFs grandes**: PodrÃ­an exceder lÃ­mites de tokens
3. **MÃºltiples PDFs**: Sin estructura para diferenciarlos

## ğŸ”§ 6. Mejoras Propuestas

### OpciÃ³n A: InyecciÃ³n Constante (Recomendada)
```python
# Inyectar PDF en cada mensaje
def chat(message, pdf_content, conversation_history):
    system_prompt_with_pdf = f"""
    {base_prompt}
    
    DOCUMENTO ACTUAL:
    ---
    {pdf_content}
    ---
    
    HISTORIAL DE CONVERSACIÃ“N:
    {conversation_history}
    
    NUEVA PREGUNTA: {message}
    """
```

### OpciÃ³n B: MÃºltiples PDFs con SegmentaciÃ³n
```python
def chat(message, pdf_contents_dict, conversation_history):
    system_prompt = f"""
    {base_prompt}
    
    DOCUMENTOS DISPONIBLES:
    
    DOCUMENTO 1 - "archivo1.pdf":
    ---
    {pdf_contents_dict['doc1']}
    ---
    
    DOCUMENTO 2 - "archivo2.pdf":
    ---
    {pdf_contents_dict['doc2']}
    ---
    
    CONVERSACIÃ“N PREVIA:
    {conversation_history}
    
    NUEVA PREGUNTA: {message}
    """
```

### OpciÃ³n C: Monitoreo de Tokens
```python
def estimate_tokens(text):
    return len(text) // 4  # AproximaciÃ³n

def chat_with_token_management(message, pdf_content, history):
    total_tokens = (
        estimate_tokens(base_prompt) +
        estimate_tokens(pdf_content) +
        estimate_tokens(str(history)) +
        estimate_tokens(message)
    )
    
    if total_tokens > MAX_TOKENS:
        # Truncar historial o segmentar PDF
        pass
```

## ğŸ¯ 7. RecomendaciÃ³n Inmediata

### Para el MVP (MÃ­nimo Producto Viable):

1. **Mantener**: 1 PDF por sesiÃ³n
2. **Implementar**: InyecciÃ³n constante del PDF
3. **Agregar**: Monitoreo bÃ¡sico de tokens
4. **Preparar**: Estructura para mÃºltiples PDFs a futuro

### CÃ³digo Propuesto:
```python
class PDFChatSession:
    def __init__(self):
        self.pdf_content = None
        self.pdf_filename = None
        self.conversation_history = []
        self.token_count = 0
    
    def load_pdf(self, pdf_path):
        # Cargar y procesar PDF
        pass
    
    async def chat(self, message):
        # Inyectar PDF + historial en cada mensaje
        pass
    
    def get_token_usage(self):
        # Monitorear uso de tokens
        pass
```

## âœ… MEJORAS IMPLEMENTADAS (v2.0)

### ğŸ¯ Cambios Realizados

1. **âœ… InyecciÃ³n constante del PDF** - IMPLEMENTADO
2. **âœ… Monitoreo de tokens** - IMPLEMENTADO
3. **âœ… GestiÃ³n robusta de sesiones** - IMPLEMENTADO
4. **âœ… Sistema preparado para API** - LISTO

### ğŸ“ Nuevos Archivos

- `pdf_chat_session.py` - Clase mejorada para manejo de sesiones
- `test_enhanced_system.py` - Tests completos del sistema mejorado

### ğŸ”§ Archivos Modificados

- `llm_client.py` - InyecciÃ³n constante + monitoreo de tokens
- `SYSTEM_DOCUMENTATION.md` - DocumentaciÃ³n actualizada

### ğŸš€ PrÃ³ximos Pasos

1. **âœ… Sistema LLM mejorado** - COMPLETADO
2. **âœ… InyecciÃ³n constante PDF** - COMPLETADO
3. **âœ… Monitoreo de tokens** - COMPLETADO
4. **ğŸ”„ Backend con FastAPI** - SIGUIENTE (base sÃ³lida lista)
5. **â³ Frontend bÃ¡sico** - Pendiente

### ğŸ’¡ CaracterÃ­sticas del Sistema v2.0

- **Modelo**: Gemini 2.0 Flash (1M tokens de contexto)
- **Enfoque**: Documentos administrativos con contexto persistente
- **Estilo**: Respuestas claras con informaciÃ³n siempre disponible
- **Contexto**: InyecciÃ³n del PDF completo en CADA mensaje
- **Monitoreo**: Tracking completo de uso de tokens
- **Sesiones**: GestiÃ³n robusta con timeouts y limpieza automÃ¡tica
- **PDFs**: Procesamiento hÃ­brido (texto estÃ¡ndar + OCR)
- **Escalabilidad**: Preparado para mÃºltiples sesiones concurrentes
